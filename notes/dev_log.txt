Synopsis of development up until Jan 2, 2025:
    Context: 
        I began this project just for fun after writing what I belive to be a pretty well optimized reverse-index search strategy using Sqlite3's C library.
        The purpose of that project was to make sure that sqlite could perform well enough for a mobile app that I'm working on. The app is pretty graphics intensive and I wanted a database that was as light (haha) as possible.

        So, I got to the end of that project and was seeing ~600 microsecond query times on my workstation, which in my mind equates to ~1200 microseconds on a mobile device. You can read more of my notes about that in my 'Search' repository here on GitHub.

        My conclusion was that, well, that was good enough. But I knew that it could be faster. 
        And, as I am currently in a lul in that mobile-development project until some design work is completed, I decided to try to make something faster than sqlite for reverse index searches, specifically.
    
    What has happened so far?
        Dec. 22, 2024 - Made the first commit for this project
            - Introduced a header file containing ~370,000 words in static, constant arrays. This header file was created using a generator program written in Python.
            - Began thinking about the nature of the header file, and that the arrays (as implimented) would be stored in the program data sector.

        Dec. (26?) 27 & 28 - Started thinking about searching and sorting algorithms. Created the RSort repository here on GitHub.
        
        Dec. 28, 2024 - 'Restarted' the project, with the words now being loaded into the stack
            - Noticed that it took 0.044 microseconds to load the entire wordlist into the stack (compiled -O3 Release, executed from within IDE)
            - Noticed that running the same binary from the console resulted in a 0.22 microsecond timespan for loading the entire wordlist into the stack

        Jan. 1, 2024 - After taking a break for several days, I came back to the project and noticed some problems.
            - While working on those problems, I noticed that adding one array to the stack containing the memory locations of the wlPart structures that had already been pushed into the stack took 3 milliseconds.
                - 3 milliseconds! I went from a 0.044 microsecond load time to a ~3000 microsecond load time.


Jan 2, 2025 - Early Morning:
    - 0.015 microsecond stack-loading time (34% improvement)
    - Everything looks good in valgrind - no memory leaks
    - Print statments look better (and there are conditional compilation statements to remove them completely from release builds)
    - Ready to start working on querying this thing!
    
    Things to remember:
        - I have not yet optimized the order in which the arrays are placed into the stack. Tentatively, I think that the following order would be optimal for fast query times:
            First: Word arrays
            Second: Index arrays
            Third: 'Skip' Index arrays
            Fourth: WordList datastructure

Jan 2, 2025:
    - Oh, hey, I forgot to write a test for all this generated stuff. Did that. Caught one thing I did wrong in the generator. Fixed it.
    - Found 1 inconsistency in the number type (uint8 vs uint16) used for word sizes in wlPart and wordList. Fixed it.
    - Thought about how to express the position of a word in the wordList as an index in a way that is interoperable with databases. (see database_interop.h)
    - Revamped the CMAKE file. 
    - NOW I'm ready to start working on querying this thing! ... wait a second.
    - I tried copying wordList into mutableWordList after loading everything into the stack in primary_thread.c. The initialization time ballooned to ~3 milliseconds again (see note from Jan 1, 2024).
      I think I might be experiancing memory alignment issues. Hmm. 

Jan 3, 2025:
    - Reassessing the following ideas and thinking about how to perfectly structure this thing:
        1.) "Accessing data in the heap is slower than accessing data on the stack because you have to follow a pointer to get there."
        2.) "The memory locations of staticly allocated variables in the program data sector are already known at runtime."
              - That's good!
        3.) "Accessing items in the stack is always the fastest - faster than the heap, and faster than the program data sector."
              - I made a mistake in this assumption. ("Accessing items in the..." != "The")
              - Did some brushing up on how the stack actually works.
    
        I've disassembled a control and test version of the program into the notes directory, and also profiled the program using perf.

        The only change to the source code that was made to arrive at the 'test' variant of the disassembly is that I added the line "getMutableCopy(&masterWordlist);" at 626 of primary_threads.c.
            - The test version assembly has A LOT more going on than the control version 
                - I have no experience working in assembly, but I do know a few instructions - and that less of them is generally better than more.
            - There are a lot more cache misses in the test version than the control version.
            - Both versions have 0 alignment faults.
            - The control version has 60 page faults, while the test version has 225.

        I think it's time to backtrack and utilize the program data segment as well as the stack.
        I've itemized the memory calculations in calc_itemized.ipynb, which will be my primary reference for choosing which memory segment to use for the various datastructures.
        At this point, I know how long it takes to load everything into the stack, and how long it takes to copy memory from the stack using that strategy, and I have a 'snapshot' of the program's assembly instructions and performance for comparison against future versions.

Jan 3, 2025 - Evening:
    - Moved "words" and "indexes" to the program data sector.
    - Left everything else (including "skip indexes") on the stack.
    - ~15 microseconds to load the stack, ~0.02 microseconds to copy the wordlist.
        - That's an improvement.
    - There's a new snapshot of the assembly instructions and performance metrics in notes

    - Highlighted output from Valgrind:
        ==14475== Thread 2:
        ==14475== Conditional jump or move depends on uninitialised value(s)
        ==14475==    at 0x40318A: getMutableCopy (in /home/tyohanan/Documents/GitHub/Search 2/build/bin/Search2)
        ==14475==    by 0x403040: init_thread (in /home/tyohanan/Documents/GitHub/Search 2/build/bin/Search2)
        ==14475==    by 0x48F4D21: start_thread (in /usr/lib64/libc.so.6)
        ==14475==    by 0x4979033: clone (in /usr/lib64/libc.so.6)
        ==14475==  Uninitialised value was created by a stack allocation
        ==14475==    at 0x40166D: init_thread (in /home/tyohanan/Documents/GitHub/Search 2/build/bin/Search2)
    
    - Hmm. What's uninitialized?
        1.) Figure that out.
        2.) Write a new test that simulates a very performance-intensive search and continue optimizing how the memory is utilized based on that (instead of copying memory.)

Jan 7, 2025:
    - Referencing https://bytes.usc.edu/cs104/wiki/valgrind, I belive the uninitialized values output from Valgrind is a non-issue.
       - masterWordlist (line 218 of primary_thread.c) 'initializes' some integer parameters and then is copied at line 446.
       - this message is no longer presented with the new tests.

    - While writing the new tests I noticed some things:
       - In WordListIsSafe() and timedFakeSearch_p2() I use a 'fancy' cast that looks fun, however, it may have some negative effects on performance.
          - This is what I mean by 'fancy' casting (if there is a technical term for this, I'm not aware of it): 
              - const uint16_t (*idxs)[wordListPartition.size][wordListPartition.ct]  = (const uint16_t (**)[wordListPartition.size][wordListPartition.ct]) wordListPartition.idxs;
          - In debugging, it seems that the program 'jumps back and forth' several times to determine values each time the 'fancy' casted variable is referenced.
          - I think it is worthwhile to test the following changes to determine whether there is a performance impact associated with 'fancy' casting before continuing on: 
              - Change wlPart.idxs and wlPart.skIdxs to ** instead of *
                - Jan 8, 2025 CORRECTION: Not **. That wouldn't make sense. No changes to wlPart.
              - Remove the 'fancy' casts and use pointer addition instead

Jan 8, 2025:
    - Modified timedFakeSearch_p2 to use pointer addition instead of 'fancy' casting. The function is called timedFakeSearch_p2_notFancy.
    - Tested it against timedFakeSearch_p2. Result: timedFakeSearch_p2_notFancy was faster.
    - Made a simple optimization to timedFakeSearch_p2. Result: timedFakeSearch_p2 was faster.
    - Conclusion: the 'jumps' observed while debugging are nothing to worry about.

Jan 9, 2025:
    - There's a new snapshot of the assembly instructions and performance metrics in notes
    
    - Next Steps:
        - Optimize memory (this is the main section of the project)
        - Write search/sort algorithms
        - Multithread
        - Turn this thing into a shared library

Jan 16, 2025:
    I have begun working on the mobile app again (see 'Synopsis of development up until Jan 2, 2025'), and also the school semester has now begun, so I'm putting this project on pause.
    I do plan on coming back to it! I want to use this, or something similar to it, for the server end of the app.

    Since Jan 9, 2025 I have written a more realistic search implimentation that uses the skip indexes.
    While doing that, I changed the skip indexes a bit: there was one problem with the generator - now fixed - and I also changed the second element of the skip index to be the number of indicies between the sorted index segments rather than the index of the next index segment.
    These changes are not yet on GitHub.

    So, here's the problem:
      Even using the skip indexes, the search takes too long.
      It takes about 600 microseconds to find all of the indexes of words that contain the letter 'e,' without filtering out duplicates.
      Filtering out duplicates could be pretty dang fast, and that's a whole project in itself. I'm thinking a modified radix sort + bsearch.
      Retrieving the words would be slow, since we have to "jump around" in memory. Another 600 microseconds? Maybe.

    So, that's fine. I think it would be faster than SQlite for this one specific type of search I'm trying to do (reference my previous project, here on GitHub, 'Search') even without multithreading, but that's not what I'm after. I want it to be WAY faster. I'm not really doing a side-by-side comparison here. I'm using way more words in this project than in the last.

    If I optimized memory further (I'm no expert on this) I would combine the skip-indexes with their corresponding index arrays so that the first 2*26 elements of the array is the skip index, and the rest is the index. That could be good for cache locality. There are a few things that could be done with SIMD instructions, but so far, not many. 

    Still, by my estimation, it would not be fast enough for what I want to use it for - even with multithreading. When I come back to this, I'm going to try doing it with a GPU. 

